{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "623882d6",
   "metadata": {},
   "source": [
    "# D3S Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e3a673",
   "metadata": {},
   "source": [
    "Author: Mohamed MESSALTI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7906072",
   "metadata": {},
   "source": [
    "Code is partly based on the following 2 articles:\n",
    "\n",
    "- [Supplier Name Standardization using Unsupervised Learning, Rahul Issar, Oct 6, 2020](https://medium.com/analytics-vidhya/supplier-name-standardization-using-unsupervised-learning-adb27bed9e0d)\n",
    "- [Company Name Standardization using a Fuzzy NLP Approach, Shashank Gupta, et al. March 28, 2020](https://www.analyticsinsight.net/company-names-standardization-using-a-fuzzy-nlp-approach/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2d4abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "# pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51092a",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0163b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Mohamed\n",
      "[nltk_data]     MESSALTI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mohamed\n",
      "[nltk_data]     MESSALTI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import unicodedata\n",
    "import re # Regular expression operations\n",
    "\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "import nltk # Natural Language Toolkit\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from fuzzywuzzy import fuzz # to compute the Levenshtein Distance\n",
    "\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "import difflib # For the SequenceMatcher to standardize company name of each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4c6e57",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We use standard data pre-processing techniques to clean textual data and remove unwanted words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc549109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords to help identify stop_words\n",
    "company_stopwords=['biz', 'bv', 'co', 'comp', 'company', \n",
    "                    'corp','corporation', 'dba', \n",
    "                    'inc', 'incorp', 'incorporat', \n",
    "                    'incorporate', 'incorporated', 'incorporation', \n",
    "                    'international', 'intl', 'intnl', \n",
    "                    'limited' ,'llc', 'ltd', 'llp', \n",
    "                    'machines', 'pvt', 'pte', 'private', 'unknown',\n",
    "                    'gmbh', 'sa', 'sarl', 'sas', 'sl', 'sal', 'university',\n",
    "                    'universite', 'group', 'association', 'agence', 'service']\n",
    "\n",
    "\n",
    "\n",
    "# Text data encoder function\n",
    "def filter_ascii(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "# Remove special characters & digits (optional) function\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "# Remove company specific stop words\n",
    "def clean_stopwords(text,eng=False):\n",
    "    if eng == False:\n",
    "        custom = company_stopwords\n",
    "    else:\n",
    "        custom = company_stopwords + list(ENGLISH_STOP_WORDS)\n",
    "    for x in custom:\n",
    "        pattern2 = r'\\b'+x+r'\\b'\n",
    "        text=re.sub(pattern2,'',text)\n",
    "    return text\n",
    "\n",
    "# Trim the text to remove spaces\n",
    "def clean_spaces(text):\n",
    "    text=text.replace('  ', ' ')\n",
    "    text=text.strip()\n",
    "    if len(text) < 1:\n",
    "        text='Tooshorttext'\n",
    "    return text\n",
    "\n",
    "# Function to Preprocess Textual data. Provide input as df['Column Name'] to this function\n",
    "def preprocess_text(column, remove_digits=True, lemm=True, eng=False):\n",
    "    try:\n",
    "        column = [filter_ascii(text) for text in column]\n",
    "        column = [remove_special_characters(text, remove_digits) for text in column]\n",
    "        column = [text.lower() for text in column] # convert all text to lower case\n",
    "        column = [clean_stopwords(text, eng) for text in column]\n",
    "        column = [clean_spaces(text) for text in column]\n",
    "        ## Lemmatisation (convert the word into root word)\n",
    "        if lemm == True:\n",
    "            lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "            column = [lem.lemmatize(text) for text in column]\n",
    "        return column\n",
    "    except Exception as e:\n",
    "        return print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381692f",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d09fc329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate similarity matrix. Provide input as df['Column Name'] to this function\n",
    "def fuzz_similarity(column):\n",
    "  similarity_array = np.ones((len(column), (len(column))))*100\n",
    "  for i in range(1, len(column)):\n",
    "    for j in range(i):\n",
    "      s1 = fuzz.token_set_ratio(column.iloc[i],column.iloc[j]) + 0.00000000001\n",
    "      s2 = fuzz.partial_ratio(column.iloc[i],column.iloc[j]) + 0.00000000001\n",
    "      similarity_array[i][j] = 2*s1*s2 / (s1+s2)\n",
    "      \n",
    "  for i in range(len(column)):\n",
    "    for j in range(i+1,len(column)):\n",
    "      similarity_array[i][j] = similarity_array[j][i]\n",
    "      np.fill_diagonal(similarity_array, 100)\n",
    "  return similarity_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936937a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to cluster similar name companies. Provide input as dataframe and numpy similarity matrix to this function\n",
    "def company_clusters(dataframe, matrix):\n",
    "    dataframe['index'] = dataframe.index \n",
    "    cust_ids = dataframe.index.to_list()\n",
    "    clusters = AffinityPropagation(affinity='precomputed').fit_predict(matrix)\n",
    "    df_clusters = pd.DataFrame(list(zip(cust_ids, clusters)), columns=['index','Cluster'])\n",
    "    new = dataframe.merge(df_clusters, 'inner', 'index')\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dc23240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map the standard name of each cluster and calculat the confidence of each row with respect to its cluster.\n",
    "# From the list of substrings for a cluster, we take the one with the highest occurrence (mode), which is considered as the Standard name to be assigned to the current cluster\n",
    "# Confidence score quantifies the confidence with which we can say that the standard name we identified truly represents the company name for the raw string.\n",
    "def standard_name(df_eval):\n",
    "    d_standard_name = {}\n",
    "    for cluster in df_eval.Cluster.unique():\n",
    "        names = df_eval[df_eval['Cluster']==cluster].Preprocessed_Text.to_list()\n",
    "        l_commun_substring = []\n",
    "        if len(names)>1:\n",
    "            for i in range(0,len(names)):\n",
    "                for j in range(i+1,len(names)):\n",
    "                    SeqMatch = difflib.SequenceMatcher(None, names[i],names[j])\n",
    "                    match = SeqMatch.find_longest_match(0, len(names[i]), 0, len(names[j]))\n",
    "                    if (match.size!=0):\n",
    "                        l_commun_substring.append(names[i][match.a: match.a + match.size].strip())\n",
    "            n = len(l_commun_substring)\n",
    "            counts = Counter(l_commun_substring)\n",
    "            get_mode = dict(counts)\n",
    "            mode = [k for k, v in get_mode.items() if v==max(list(counts.values()))]\n",
    "            d_standard_name[cluster] = \";\".join(mode)\n",
    "        else:\n",
    "            d_standard_name[cluster] = names[0]\n",
    "    df_standard_names = pd.DataFrame(list(d_standard_name.items()), columns=['Cluster', 'Mapped_name'])\n",
    "    df_eval = df_eval.merge(df_standard_names, on='Cluster', how='left')\n",
    "    \n",
    "    df_eval['Confidence'] = df_eval.apply(lambda x: fuzz.token_set_ratio(x['Mapped_name'], x['Preprocessed_Text']),axis=1)\n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf320ff",
   "metadata": {},
   "source": [
    "## Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14904335",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00b610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Case_study_names_mapping_test.xlsx\")\n",
    "df = df.drop(['Mapped name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3fdcf",
   "metadata": {},
   "source": [
    "If you want to run a random subsample, please run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00acb08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123) # Fixed seed, so that the experiment can be reproduced.\n",
    "# subsample_size=100 # number of raw\n",
    "# df = df.loc[np.random.choice(df.index, subsample_size, replace=False)].copy(deep=True)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a586d843",
   "metadata": {},
   "source": [
    "### Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6384341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Preprocessed_Text'] = preprocess_text(column= df['Raw name'], remove_digits=False, lemm=True, eng=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7c8de",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a217a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The execution time to compute the similarity matrix for the data set of size  4895   is : 1445896.1153030396 ms\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "similarity_matrix = fuzz_similarity(column=df['Preprocessed_Text'])\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"The execution time to compute the similarity matrix for the data set of size \",\n",
    "      df.shape[0],\"  is :\", (end_time-start_time) * 10**3, \"ms\")\n",
    "\n",
    "# The execution time to compute the similarity matrix for a data set of size  100   is approx. :     347.46742248535156 ms\n",
    "# The execution time to compute the similarity matrix for a data set of size  1000  is approx. :   41651.59487724304 ms = 41 seconds\n",
    "# The execution time to compute the similarity matrix for a data set of size  4895  is approx. : 1383496.4966773987 ms = 23 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f0650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = company_clusters(df, similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b52c9096",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = standard_name(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0d0f3",
   "metadata": {},
   "source": [
    "### Exporting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6186cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"results.xlsx\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
